{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6429f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer, BertModel, TimesformerForVideoClassification, AutoImageProcessor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else\n",
    "                      \"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5652785e",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "537256f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tag_vocab(tag_lists, min_freq=1):\n",
    "    tag_freq = defaultdict(int)\n",
    "    for tags in tag_lists:\n",
    "        for tag in tags:\n",
    "            tag_freq[tag.lower()] += 1\n",
    "\n",
    "    vocab = {'[PAD]': 0, '[UNK]': 1}\n",
    "    for tag, freq in tag_freq.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[tag] = len(vocab)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "df_train = pd.read_csv('movie_data_train.csv')\n",
    "\n",
    "df_train['title_overview'] = df_train['original_title'] + ': ' + df_train['overview']\n",
    "\n",
    "df_train = pd.DataFrame({\n",
    "    'title_overview': df_train['title_overview'],\n",
    "    'tags': df_train['tags'].fillna(''),\n",
    "    'revenue': df_train['revenue']\n",
    "})\n",
    "\n",
    "df_test = pd.read_csv('movie_data_test.csv')\n",
    "\n",
    "df_test['title_overview'] = df_test['original_title'] + ': ' + df_test['overview']\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    'title_overview': df_test['title_overview'],\n",
    "    'tags': df_test['tags'].fillna(''),\n",
    "    'revenue': df_test['revenue']\n",
    "})\n",
    "\n",
    "df_train['revenue'] = np.log1p(df_train['revenue'])\n",
    "df_test['revenue'] = np.log1p(df_test['revenue'])\n",
    "\n",
    "df_train['tags'] = df_train['tags'].apply(lambda x: [tag.strip().lower() for tag in x.split(',') if tag.strip()])\n",
    "df_test['tags'] = df_test['tags'].apply(lambda x: [tag.strip().lower() for tag in x.split(',') if tag.strip()])\n",
    "\n",
    "train_texts = df_train['title_overview'].tolist()\n",
    "train_tags = df_train['tags'].tolist()\n",
    "train_targets = df_train['revenue'].tolist()\n",
    "\n",
    "test_texts = df_test['title_overview'].tolist()\n",
    "test_tags = df_test['tags'].tolist()\n",
    "test_targets = df_test['revenue'].tolist()\n",
    "\n",
    "tag_vocab = build_tag_vocab(train_tags + test_tags)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39b25f9",
   "metadata": {},
   "source": [
    "## Prepare Title/Overview + Tag 2 Tower BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a96b3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TagCNNEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=300, num_filters=128, kernel_sizes=(2, 3, 4), dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(embed_dim, num_filters, k) for k in kernel_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x).transpose(1, 2)\n",
    "        conv_outs = [torch.relu(conv(embedded)).max(dim=2)[0] for conv in self.convs]\n",
    "        out = torch.cat(conv_outs, dim=1)\n",
    "        return self.dropout(out)\n",
    "\n",
    "class BERTWithTagCNNRegressor(nn.Module):\n",
    "    def __init__(self, tag_vocab_size, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.tag_encoder = TagCNNEncoder(tag_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size + 384, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, text_input_ids, text_attention_mask, tag_token_ids):\n",
    "\n",
    "        bert_output = self.bert(input_ids=text_input_ids, attention_mask=text_attention_mask)\n",
    "        text_cls = bert_output.pooler_output  \n",
    "\n",
    "        tag_feat = self.tag_encoder(tag_token_ids)\n",
    "\n",
    "        fused = torch.cat([text_cls, tag_feat], dim=1)\n",
    "        return self.regressor(self.dropout(fused))\n",
    "    \n",
    "class MovieDatasetWithTags(nn.Module):\n",
    "    def __init__(self, texts, tags, targets, tokenizer, tag_vocab, max_text_len=256, max_tag_len=20):\n",
    "        self.texts = texts\n",
    "        self.tags = tags\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tag_vocab = tag_vocab\n",
    "        self.max_text_len = max_text_len\n",
    "        self.max_tag_len = max_tag_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def encode_tags(self, tag_list):\n",
    "        tag_ids = [self.tag_vocab.get(tag.lower(), self.tag_vocab['[UNK]']) for tag in tag_list]\n",
    "        tag_ids = tag_ids[:self.max_tag_len]\n",
    "        tag_ids += [self.tag_vocab['[PAD]']] * (self.max_tag_len - len(tag_ids))\n",
    "        return torch.tensor(tag_ids, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        tags = self.tags[idx]\n",
    "        target = torch.tensor(self.targets[idx], dtype=torch.float)\n",
    "\n",
    "        text_enc = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_text_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        tag_tensor = self.encode_tags(tags)\n",
    "\n",
    "        return {\n",
    "            'input_ids': text_enc['input_ids'].squeeze(0),\n",
    "            'attention_mask': text_enc['attention_mask'].squeeze(0),\n",
    "            'tags': tag_tensor,\n",
    "            'target': target\n",
    "        }\n",
    "\n",
    "train_texts_split, val_texts, train_tags_split, val_tags, train_targets_split, val_targets = train_test_split(\n",
    "    train_texts, train_tags, train_targets, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = MovieDatasetWithTags(train_texts_split, train_tags_split, train_targets_split, tokenizer, tag_vocab)\n",
    "val_dataset = MovieDatasetWithTags(val_texts, val_tags, val_targets, tokenizer, tag_vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "test_dataset = MovieDatasetWithTags(test_texts, test_tags, test_targets, tokenizer, tag_vocab)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f8530e",
   "metadata": {},
   "source": [
    "## Get Predictions of 2 Tower BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d5dd63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTWithTagCNNRegressor(tag_vocab_size=len(tag_vocab)).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "checkpoint = torch.load('models/title_overview_two_tower_model.pt', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "title_train_predictions = []\n",
    "train_actuals = []\n",
    "title_val_predictions = []\n",
    "val_actuals = []\n",
    "title_test_predictions = []\n",
    "test_actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        tags = batch['tags'].to(device)\n",
    "        targets = batch['target'].cpu().numpy()\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask, tags).squeeze().cpu().numpy()\n",
    "        \n",
    "        title_train_predictions.extend(outputs)\n",
    "        train_actuals.extend(targets)\n",
    "\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        tags = batch['tags'].to(device)\n",
    "        targets = batch['target'].cpu().numpy()\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask, tags).squeeze().cpu().numpy()\n",
    "        \n",
    "        title_val_predictions.extend(outputs)\n",
    "        val_actuals.extend(targets)\n",
    "\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        tags = batch['tags'].to(device)\n",
    "        targets = batch['target'].cpu().numpy()\n",
    "\n",
    "        outputs = model(input_ids, attention_mask, tags).squeeze().cpu().numpy()\n",
    "\n",
    "        title_test_predictions.extend(outputs)\n",
    "        test_actuals.extend(targets)\n",
    "        \n",
    "        \n",
    "\n",
    "title_train_preds = np.expm1(title_train_predictions)\n",
    "train_actuals = np.expm1(train_actuals)\n",
    "title_val_preds = np.expm1(title_val_predictions)\n",
    "val_actuals = np.expm1(val_actuals)\n",
    "title_test_preds = np.expm1(title_test_predictions)\n",
    "test_actuals = np.expm1(test_actuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcaa44a",
   "metadata": {},
   "source": [
    "## Setting Up Visual Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15e5a7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiImageDataset(Dataset):\n",
    "    def __init__(self, df, poster_dir, backdrop_dir, thumbnail_dir, transform):\n",
    "        self.df = df\n",
    "        self.poster_dir = poster_dir\n",
    "        self.backdrop_dir = backdrop_dir\n",
    "        self.thumbnail_dir = thumbnail_dir\n",
    "        self.transform = transform\n",
    "        self.valid_ids = []\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            movie_id = str(int(row['id']))\n",
    "            if all(os.path.exists(os.path.join(d, f\"{movie_id}.jpg\")) for d in [poster_dir, backdrop_dir, thumbnail_dir]):\n",
    "                self.valid_ids.append(idx)\n",
    "            else:\n",
    "                self.valid_ids.append(-1)                \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if self.valid_ids[idx] == -1:\n",
    "            return {\n",
    "                \"poster\": torch.zeros(3, 224, 224),\n",
    "                \"backdrop\": torch.zeros(3, 224, 224),\n",
    "                \"thumbnail\": torch.zeros(3, 224, 224),\n",
    "                \"revenue\": torch.tensor(0, dtype=torch.float)\n",
    "            }\n",
    "        \n",
    "        df_idx = self.valid_ids[idx]\n",
    "        row = self.df.iloc[df_idx]\n",
    "        movie_id = str(int(row['id']))\n",
    "        revenue = np.log1p(row['revenue'])\n",
    "\n",
    "        def load_image(directory):\n",
    "            image = Image.open(os.path.join(directory, f\"{movie_id}.jpg\")).convert(\"RGB\")\n",
    "            return self.transform(image)\n",
    "\n",
    "        return {\n",
    "            \"poster\": load_image(self.poster_dir),\n",
    "            \"backdrop\": load_image(self.backdrop_dir),\n",
    "            \"thumbnail\": load_image(self.thumbnail_dir),\n",
    "            \"revenue\": torch.tensor(revenue, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "def get_resnet_backbone():\n",
    "    resnet = models.resnet50(pretrained=True)\n",
    "    for param in resnet.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in resnet.layer4.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in resnet.avgpool.parameters():\n",
    "        param.requires_grad = True\n",
    "    return nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "class FineTunedEnsemble(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.poster_net = get_resnet_backbone()\n",
    "        self.backdrop_net = get_resnet_backbone()\n",
    "        self.thumbnail_net = get_resnet_backbone()\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2048*3, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, poster, backdrop, thumbnail):\n",
    "        p = self.poster_net(poster)\n",
    "        b = self.backdrop_net(backdrop)\n",
    "        t = self.thumbnail_net(thumbnail)\n",
    "\n",
    "        x = torch.cat([p.view(p.size(0), -1), b.view(b.size(0), -1), t.view(t.size(0), -1)], dim=1)\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ad2f81",
   "metadata": {},
   "source": [
    "## Preparing Data For Visuals Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b84b0a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "df_train = pd.read_csv(\"movie_data_train.csv\")\n",
    "df_test = pd.read_csv(\"movie_data_test.csv\")\n",
    "\n",
    "train_dataset = MultiImageDataset(df_train, \"poster_dataset\", \"backdrop_dataset\", \"thumbnail_dataset\", transform)\n",
    "test_dataset = MultiImageDataset(df_test, \"poster_dataset\", \"backdrop_dataset\", \"thumbnail_dataset\", transform)\n",
    "\n",
    "train_idx, val_idx = train_test_split(list(range(len(train_dataset))), test_size=0.2, random_state=42)\n",
    "train_subset = torch.utils.data.Subset(train_dataset, train_idx)\n",
    "val_subset = torch.utils.data.Subset(train_dataset, val_idx)\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf946e1",
   "metadata": {},
   "source": [
    "## Get Predictions from Visuals Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3092adcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nigelpoh/Desktop/SUTD/Computational Data Science/project_github/proj_env/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/nigelpoh/Desktop/SUTD/Computational Data Science/project_github/proj_env/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Loading Training Predictions: 100%|██████████| 175/175 [00:40<00:00,  4.30it/s]\n",
      "Loading Validation Predictions: 100%|██████████| 44/44 [00:10<00:00,  4.36it/s]\n",
      "Loading Test Predictions: 100%|██████████| 55/55 [00:12<00:00,  4.44it/s]\n"
     ]
    }
   ],
   "source": [
    "model = FineTunedEnsemble().to(device)\n",
    "checkpoint = torch.load('models/best_ensemble_model.pt', map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "visuals_train_preds = []\n",
    "visuals_val_preds = []\n",
    "visuals_test_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(train_loader, desc=\"Loading Training Predictions\"):\n",
    "        p = batch[\"poster\"].to(device)\n",
    "        b = batch[\"backdrop\"].to(device)\n",
    "        t = batch[\"thumbnail\"].to(device)\n",
    "        y = batch[\"revenue\"].to(device)\n",
    "        y_hat = model(p, b, t).squeeze()\n",
    "        visuals_train_preds.extend(y_hat.cpu().view(-1).tolist())\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Loading Validation Predictions\"):\n",
    "        p = batch[\"poster\"].to(device)\n",
    "        b = batch[\"backdrop\"].to(device)\n",
    "        t = batch[\"thumbnail\"].to(device)\n",
    "        y = batch[\"revenue\"].to(device)\n",
    "        y_hat = model(p, b, t).squeeze()\n",
    "        visuals_val_preds.extend(y_hat.cpu().view(-1).tolist())\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Loading Test Predictions\"):\n",
    "        p = batch[\"poster\"].to(device)\n",
    "        b = batch[\"backdrop\"].to(device)\n",
    "        t = batch[\"thumbnail\"].to(device)\n",
    "        y = batch[\"revenue\"].to(device)\n",
    "        y_hat = model(p, b, t).squeeze()\n",
    "        visuals_test_preds.extend(y_hat.cpu().view(-1).tolist())\n",
    "\n",
    "visuals_train_preds = np.expm1(visuals_train_preds)\n",
    "visuals_val_preds = np.expm1(visuals_val_preds)\n",
    "visuals_test_preds = np.expm1(visuals_test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9773a68f",
   "metadata": {},
   "source": [
    "## Setting Up Trailer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5adea36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_sort_key(filename):\n",
    "    return [int(text) if text.isdigit() else text.lower()\n",
    "            for text in re.split(r'(\\d+)', filename)]\n",
    "\n",
    "class MovieKeyframeDataset(Dataset):\n",
    "    def __init__(self, dataframe, frame_dir, image_processor, num_frames=8):\n",
    "        self.dataframe = dataframe\n",
    "        self.frame_dir = frame_dir\n",
    "        self.image_processor = image_processor\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        trailer_id = row['trailer']\n",
    "        label = torch.tensor(row['log_revenue'], dtype=torch.float32)\n",
    "\n",
    "        frame_folder = os.path.join(self.frame_dir, str(trailer_id))\n",
    "\n",
    "        if not os.path.exists(frame_folder):\n",
    "\n",
    "            dummy_frames = torch.zeros((self.num_frames, 3, 224, 224))\n",
    "            return {\n",
    "                \"pixel_values\": dummy_frames,\n",
    "                \"labels\": label\n",
    "            }\n",
    "\n",
    "        frame_files = sorted([\n",
    "            f for f in os.listdir(frame_folder) if f.endswith(\".jpg\")\n",
    "        ], key=numerical_sort_key)\n",
    "\n",
    "        selected_frames = frame_files[3:self.num_frames+3]\n",
    "\n",
    "        frames = []\n",
    "        for fname in selected_frames:\n",
    "            img_path = os.path.join(frame_folder, fname)\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            frames.append(np.array(img))\n",
    "\n",
    "        pixel_values = self.image_processor(frames, return_tensors=\"pt\")[\"pixel_values\"][0]\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": label             \n",
    "        }\n",
    "    \n",
    "\n",
    "\n",
    "class TimeSformer(nn.Module):\n",
    "    def __init__(self, model_name=\"facebook/timesformer-base-finetuned-k400\"):\n",
    "        super().__init__()\n",
    "        self.backbone = TimesformerForVideoClassification.from_pretrained(model_name)\n",
    "\n",
    "        hidden_size = self.backbone.config.hidden_size \n",
    "\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values): \n",
    "        outputs = self.backbone(pixel_values)\n",
    "        return outputs.logits.view(-1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab86d4ea",
   "metadata": {},
   "source": [
    "## Loading Data for Trailer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "addae257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoImageProcessor.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n",
    "\n",
    "\n",
    "df_train = pd.read_csv('movie_data_train.csv')\n",
    "df_train['log_revenue'] = np.log1p(df_train['revenue'])\n",
    "\n",
    "train_dataset, val_dataset = train_test_split(df_train, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = MovieKeyframeDataset(train_dataset, \"frames\", processor, num_frames=8)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "val_dataset = MovieKeyframeDataset(val_dataset, \"frames\", processor, num_frames=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "df_test = pd.read_csv('movie_data_test.csv')\n",
    "df_test['log_revenue'] = np.log1p(df_test['revenue'])\n",
    "test_dataset = MovieKeyframeDataset(df_test, \"frames\", processor, num_frames=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bb5fda",
   "metadata": {},
   "source": [
    "## Get Predictions from Trailer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bf3b74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Training Predictions: 100%|██████████| 2786/2786 [05:11<00:00,  8.93it/s]\n",
      "Loading Validation Predictions: 100%|██████████| 697/697 [01:20<00:00,  8.62it/s]\n",
      "Loading Test Predictions: 100%|██████████| 871/871 [01:39<00:00,  8.79it/s]\n"
     ]
    }
   ],
   "source": [
    "model = TimeSformer().to(device) \n",
    "checkpoint = torch.load('models/best_trailer_model.pt', map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()\n",
    "\n",
    "trailer_train_preds = []\n",
    "trailer_val_preds = []\n",
    "trailer_test_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(train_loader, desc=\"Loading Training Predictions\"):\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(pixel_values)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        trailer_train_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    for batch in tqdm(val_loader, desc=\"Loading Validation Predictions\"):\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(pixel_values)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        trailer_val_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    for batch in tqdm(test_loader, desc=\"Loading Test Predictions\"):\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(pixel_values)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        trailer_test_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "\n",
    "trailer_train_preds = np.expm1(trailer_train_preds)\n",
    "trailer_val_preds = np.expm1(trailer_val_preds)\n",
    "trailer_test_preds = np.expm1(trailer_test_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630c10f3",
   "metadata": {},
   "source": [
    "## Prepare the Dataset for Generic Features Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7e70d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape: (871, 26)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('movie_data_train.csv')\n",
    "\n",
    "numerical_features = ['budget', 'runtime', 'viewCount', 'likeCount', 'favoriteCount', 'commentCount']\n",
    "\n",
    "df['release_month'] = pd.to_datetime(df['release_date']).dt.month\n",
    "\n",
    "df['genres_list'] = df['genres'].str.split(',')\n",
    "\n",
    "X_numeric = df[numerical_features + ['release_month']]\n",
    "y = df['revenue']\n",
    "\n",
    "genres_exploded = df['genres_list'].explode()\n",
    "unique_genres = genres_exploded.dropna().unique()\n",
    "\n",
    "for genre in unique_genres:\n",
    "    df[f'genre_{genre.strip()}'] = df['genres_list'].apply(\n",
    "        lambda x: 1 if x is not None and genre in [g.strip() for g in x] else 0\n",
    "    )\n",
    "\n",
    "genre_columns = [col for col in df.columns if col.startswith('genre_')]\n",
    "\n",
    "X_combined = pd.concat([X_numeric, df[genre_columns]], axis=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_combined, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_val = imputer.transform(X_val)\n",
    "\n",
    "test_df = pd.read_csv('movie_data_test.csv')\n",
    "\n",
    "test_df['release_month'] = pd.to_datetime(test_df['release_date']).dt.month\n",
    "test_df['genres_list'] = test_df['genres'].str.split(',')\n",
    "\n",
    "X_test_numeric = test_df[numerical_features + ['release_month']]\n",
    "\n",
    "for genre in unique_genres:\n",
    "    test_df[f'genre_{genre.strip()}'] = test_df['genres_list'].apply(\n",
    "        lambda x: 1 if x is not None and genre in [g.strip() for g in x] else 0\n",
    "    )\n",
    "\n",
    "X_test = pd.concat([X_test_numeric, test_df[genre_columns]], axis=1)\n",
    "\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "X_test = SimpleImputer(strategy='constant', fill_value=0).fit_transform(X_test)\n",
    "\n",
    "y_test = test_df['revenue']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12491d7",
   "metadata": {},
   "source": [
    "## Get Predictions from Generic Features Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b62aa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBRegressor(\n",
    "            n_estimators=2000,\n",
    "            learning_rate=0.015,\n",
    "            max_leaves=10,\n",
    "            subsample=0.5,\n",
    "            colsample_bytree=0.6,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=0.1,\n",
    "            random_state=42,\n",
    "            min_child_weight=40,\n",
    "            tree_method=\"hist\", \n",
    "            verbosity=0\n",
    "        )\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "generic_train_preds = model.predict(X_train)\n",
    "generic_val_preds = model.predict(X_val)\n",
    "generic_test_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82b18d7",
   "metadata": {},
   "source": [
    "## Preparing Data for Late Fusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10808419",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got Tensor)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m val_actuals = val_actuals.reshape(-\u001b[32m1\u001b[39m,\u001b[32m1\u001b[39m)\n\u001b[32m     19\u001b[39m test_actuals = test_actuals.reshape(-\u001b[32m1\u001b[39m,\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m train_actuals = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_actuals\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m     22\u001b[39m val_actuals = torch.from_numpy(val_actuals).to(device)\n\u001b[32m     23\u001b[39m test_actuals = torch.from_numpy(test_actuals).to(device)\n",
      "\u001b[31mTypeError\u001b[39m: expected np.ndarray (got Tensor)"
     ]
    }
   ],
   "source": [
    "title_train_preds = title_train_preds.reshape(-1, 1)\n",
    "title_val_preds = title_val_preds.reshape(-1, 1)\n",
    "title_test_preds = title_test_preds.reshape(-1, 1)\n",
    "\n",
    "visuals_train_preds = visuals_train_preds.reshape(-1, 1)\n",
    "visuals_val_preds = visuals_val_preds.reshape(-1, 1)\n",
    "visuals_test_preds = visuals_test_preds.reshape(-1, 1)\n",
    "\n",
    "trailer_train_preds = trailer_train_preds.reshape(-1, 1)\n",
    "trailer_val_preds = trailer_val_preds.reshape(-1, 1)\n",
    "trailer_test_preds = trailer_test_preds.reshape(-1, 1)\n",
    "\n",
    "generic_train_preds = generic_train_preds.reshape(-1, 1)\n",
    "generic_val_preds = generic_val_preds.reshape(-1, 1)\n",
    "generic_test_preds = generic_test_preds.reshape(-1, 1)\n",
    "\n",
    "train_actuals = train_actuals.reshape(-1,1)\n",
    "val_actuals = val_actuals.reshape(-1,1)\n",
    "test_actuals = test_actuals.reshape(-1,1)\n",
    "\n",
    "train_actuals = torch.from_numpy(train_actuals).to(device)\n",
    "val_actuals = torch.from_numpy(val_actuals).to(device)\n",
    "test_actuals = torch.from_numpy(test_actuals).to(device)\n",
    "\n",
    "train_preds = np.concatenate([title_train_preds, visuals_train_preds, trailer_train_preds, generic_train_preds], axis=1)\n",
    "val_preds = np.concatenate([title_val_preds, visuals_val_preds, trailer_val_preds, generic_val_preds], axis=1)\n",
    "test_preds = np.concatenate([title_test_preds, visuals_test_preds, trailer_test_preds, generic_test_preds], axis=1)\n",
    "\n",
    "train_preds = torch.from_numpy(train_preds.astype(np.float32)).to(device)\n",
    "val_preds = torch.from_numpy(val_preds.astype(np.float32)).to(device)\n",
    "test_preds = torch.from_numpy(test_preds.astype(np.float32)).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d95873ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatingNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[32, 16]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        dims = [input_dim] + hidden_dims\n",
    "\n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.extend([\n",
    "                nn.Linear(dims[i], dims[i + 1]),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "        layers.append(nn.Linear(dims[-1], input_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x)\n",
    "        weights = F.softmax(logits, dim=1)\n",
    "        return weights\n",
    "\n",
    "\n",
    "class GatingFusionEnsemble:\n",
    "    def __init__(self, lr=1e-3, epochs=100, logging=True):\n",
    "        self.gating_net = None\n",
    "        self.logging = logging\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        input_dim = X_train.shape[1]\n",
    "        self.gating_net = GatingNetwork(input_dim).to(X_train.device)\n",
    "        optimizer = torch.optim.Adam(self.gating_net.parameters(), lr=self.lr)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            self.gating_net.train()\n",
    "            optimizer.zero_grad()\n",
    "            weights = self.gating_net(X_train)\n",
    "            weighted_preds = torch.sum(X_train * weights, dim=1, keepdim=True)\n",
    "            loss = criterion(weighted_preds, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if X_val is not None and y_val is not None:\n",
    "                self.gating_net.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_weights = self.gating_net(X_val)\n",
    "                    val_preds = torch.sum(X_val * val_weights, dim=1, keepdim=True)\n",
    "                    val_loss = criterion(val_preds, y_val)\n",
    "\n",
    "                    val_r2 = r2_score(y_val.cpu().numpy(), val_preds.cpu().numpy())\n",
    "\n",
    "                    if self.logging:\n",
    "                        print(f\"Epoch {epoch} | Train Loss: {loss.item():.4f} | Val Loss: {val_loss.item():.4f} | Val R²: {val_r2:.4f}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.gating_net.eval()\n",
    "        with torch.no_grad():\n",
    "            weights = self.gating_net(X)\n",
    "            return torch.sum(X * weights, dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e84dbdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss: 0.9471 | Val Loss: 0.7622 | Val R²: 0.4538\n",
      "Epoch 1 | Train Loss: 0.9470 | Val Loss: 0.7623 | Val R²: 0.4538\n",
      "Epoch 2 | Train Loss: 0.9469 | Val Loss: 0.7623 | Val R²: 0.4538\n",
      "Epoch 3 | Train Loss: 0.9468 | Val Loss: 0.7623 | Val R²: 0.4538\n",
      "Epoch 4 | Train Loss: 0.9467 | Val Loss: 0.7623 | Val R²: 0.4538\n",
      "Epoch 5 | Train Loss: 0.9467 | Val Loss: 0.7623 | Val R²: 0.4538\n",
      "Epoch 6 | Train Loss: 0.9466 | Val Loss: 0.7623 | Val R²: 0.4538\n",
      "Epoch 7 | Train Loss: 0.9465 | Val Loss: 0.7624 | Val R²: 0.4538\n",
      "Epoch 8 | Train Loss: 0.9464 | Val Loss: 0.7624 | Val R²: 0.4538\n",
      "Epoch 9 | Train Loss: 0.9463 | Val Loss: 0.7624 | Val R²: 0.4537\n",
      "Validation R²: 0.4537\n",
      "Test R²: 0.4352\n"
     ]
    }
   ],
   "source": [
    "X_scaler = StandardScaler()\n",
    "X_train = torch.tensor(X_scaler.fit_transform(train_preds.cpu().numpy()), dtype=torch.float32).to(device)\n",
    "X_val = torch.tensor(X_scaler.transform(val_preds.cpu().numpy()), dtype=torch.float32).to(device)\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_train = torch.tensor(y_scaler.fit_transform(train_actuals.cpu().numpy()), dtype=torch.float32).to(device)\n",
    "y_val = torch.tensor(y_scaler.transform(val_actuals.cpu().numpy()), dtype=torch.float32).to(device)\n",
    "\n",
    "X_test = torch.tensor(X_scaler.transform(test_preds.cpu().numpy()), dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_scaler.transform(test_actuals.cpu().numpy()), dtype=torch.float32).to(device)\n",
    "\n",
    "gated_ensemble = GatingFusionEnsemble(lr = 1e-5, epochs=10)\n",
    "gated_ensemble.fit(X_train, y_train, X_val, y_val)\n",
    "\n",
    "with torch.no_grad():\n",
    "    val_pred = gated_ensemble.predict(X_val)\n",
    "    val_pred_np = val_pred.cpu().numpy()\n",
    "    val_actuals_np = y_val.cpu().numpy()\n",
    "    val_r2 = r2_score(val_actuals_np, val_pred_np)\n",
    "\n",
    "    test_pred = gated_ensemble.predict(X_test)\n",
    "    test_pred_np = test_pred.cpu().numpy()\n",
    "    test_actuals_np = y_test.cpu().numpy()\n",
    "    test_r2 = r2_score(test_actuals_np, test_pred_np)\n",
    "\n",
    "    print(f\"Validation R²: {val_r2:.4f}\")\n",
    "    print(f\"Test R²: {test_r2:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
